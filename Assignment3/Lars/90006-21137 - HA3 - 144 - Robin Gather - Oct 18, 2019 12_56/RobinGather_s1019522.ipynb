{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "## Objective of this assignment\n",
    "The objective of this exercise is to become familiar with fitting decision trees and\n",
    "making ROC curves in Python.\n",
    "\n",
    "\n",
    "## ** Important: ** When handing in your homework:\n",
    "+ Hand in the notebook **(and nothing else)** named as follows: StudentName1_snumber_StudentName2_snumber.ipynb\n",
    "+ Provide clear and complete answers to the questions below under a separate header (not hidden somewhere in your source code), and make sure to explain your answers / motivate your choices. Add Markdown cells where necessary.\n",
    "+ Source code, output graphs, derivations, etc., should be included in the notebook.\n",
    "+ Hand-in: upload to Brightspace.\n",
    "+ Include name, student number, assignment (especially in filenames)!\n",
    "+ When working in pairs only one of you should upload the assignment, and report the name of your partner in your filename.\n",
    "+ Use the Brightspace discussion board or email the student assistants for questions on how to complete the exercises.\n",
    "+ If you find mistakes/have suggestions/would like to complain about the assigment material itself, please email me [Lisa] at `l.tostrams@science.ru.nl`\n",
    "\n",
    "\n",
    "## Advised Reading and Exercise Material\n",
    "**The following reading material is recommended:**\n",
    "\n",
    "Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, Introduction to Data Mining, section 4.1-4.6, as well as the included article on ROC curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Decision trees\n",
    "\n",
    "In this part of the exercise we will fit decision trees using the scikitlearn classifier\n",
    "`sklearn.tree.DecisionTreeClassifier`. As a splitting criterion, the function uses\n",
    "one of the following two impurity measures:\n",
    "\n",
    "\\begin{equation} \\textrm{gdi}(t) = -\\sum_{i=1}^C p(i|t)^2 \\quad \\text{       equivalent to Gini(t)} \\end{equation}   \n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{deviance}(t) = -2\\sum_{i=1}^C p(i|t) \\log p(i|t) \\quad \\text{equivalent to Entropy(t)} \n",
    "\\end{equation}\n",
    "\n",
    "We will analyze the wine data we have used previously. The wine data set has the following attributes, all of which are continuous: \n",
    "\n",
    "\n",
    "| #             | Attribute     | Unit  |\n",
    "| ------------- |:-------------| -----:|\n",
    "|1 | Fixed acidity (tartaric) | g/dm$^3$ |\n",
    "|2 | Volatile acidity (acetic) | g/dm$^3$ |\n",
    "|3 | Citric acid | g/dm$^3$ |\n",
    "|4 | Residual sugar | g/dm$^3$ |\n",
    "|5 | Chlorides | g/dm$^3$ |\n",
    "|6 | Free sulfur dioxide | mg/dm$^3$ |\n",
    "|7 | Total sulfur dioxide | mg/dm$^3$ |\n",
    "|8 | Density | g/cm$^3$ |\n",
    "|9 | pH | pH |\n",
    "|10 | Sulphates | g/dm$^3$ |\n",
    "|11 | Alcohol | % vol. | \n",
    "\n",
    "\n",
    "#### 3.1.1\n",
    "(0.5 pts) Load the wine data set `Data/wine.mat` using the `scipy.io` `loadmat` function. This contains the same data as used in the earlier assignment, but with outliers and the 12th attribute already removed. Create data matrix $X$, class vector $y$, and the lists $attributeNames$ and $classNames$ with the data provided in the `wine.mat` file. Print out the  $attributeNames$ and $classNames$ to make sure it's a list of strings.\n",
    "\n",
    "   ** hints: **   \n",
    "*The object in wine.mat is a dictionary. The attributes are stored in matrix $X$, the class in vector $y$. $y$ is shaped as an array containing single element arrays. To flatten $y$, you can use `y.ravel()`, which unravels matrices into a 1d array. Attribute names and class names are stored in the attributeNames and classNames objects, which contain arrays, of which the first element contains the names. To get the names from those arrays, you can use list comprehension or for-loops. For more on list comprehension, check https://www.digitalocean.com/community/tutorials/understanding-list-comprehensions-in-python-3. For example, if you have an larger array containing nested arrays of which you want the first element, you can try `new_array = [nested_array[0] for nested_array in larger_array]`.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4   0.7   0.   ...  3.51  0.56  9.4 ]\n",
      " [ 7.8   0.88  0.   ...  3.2   0.68  9.8 ]\n",
      " [ 7.8   0.76  0.04 ...  3.26  0.65  9.8 ]\n",
      " ...\n",
      " [ 6.5   0.24  0.19 ...  2.99  0.46  9.4 ]\n",
      " [ 5.5   0.29  0.3  ...  3.34  0.38 12.8 ]\n",
      " [ 6.    0.21  0.38 ...  3.26  0.32 11.8 ]]\n",
      "[0 0 0 ... 1 1 1]\n",
      "['Fixed acidity' 'Volatile acidity' 'Citric acid' 'Residual sugar'\n",
      " 'Chlorides' 'Free sulfur dioxide' 'Total sulfur dioxide' 'Density' 'pH'\n",
      " 'Sulphates' 'Alcohol']\n",
      "['Red' 'White']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data = scipy.io.loadmat(\"Data/wine.mat\")\n",
    "X = np.asarray(data['X'])\n",
    "y = np.asarray(data['y']).ravel()\n",
    "attributeNamesData = data['attributeNames'][0]\n",
    "classNamesData = data['classNames'][:,0]\n",
    "\n",
    "attributeNames = np.empty(len(attributeNamesData), dtype='U25')\n",
    "for index in range(len(attributeNamesData)):\n",
    "    attributeNames[index] = attributeNamesData[index][0]\n",
    "classNames = np.empty(len(classNamesData), dtype='U10')\n",
    "for index in range(len(classNamesData)):\n",
    "    classNames[index] = classNamesData[index][0]\n",
    "    \n",
    "print(X)\n",
    "print(y)\n",
    "print(attributeNames)\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2\n",
    " (1.5 pts) Fit a decision tree to the wine data in order to estimate if the wine is red or white. Use the Gini (gdi) splitting criterion. Use `min_samples_split=100` for the stopping criterion. Explain what happens when you change the values of the parameter `min_samples_split`. After fitting the tree, visualize it with the tree_print function in `treeprint.py` in the Toolbox folder. \n",
    "\n",
    "** hints: **   \n",
    "*The `treeprint.py` file contains some documentation on how to use it. Helpfull documentation for the DecisionTreeClassifier function can be found at * http://scikit-learn.org/stable/modules/tree.html#classification  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |->3  White\n",
      "         |\n",
      "      |->2 then if Sulphates =< 0.56: go to 3, else go to 4\n",
      "      |  |\n",
      "      |  |->4  White\n",
      "      |\n",
      "   |->1 then if Chlorides =< 0.05: go to 2, else go to 5\n",
      "   |  |\n",
      "   |  |  |->6  White\n",
      "   |  |  |\n",
      "   |  |->5 else if Sulphates =< 0.37: go to 6, else go to 7\n",
      "   |     |\n",
      "   |     |  |->8  White\n",
      "   |     |  |\n",
      "   |     |->7 else if Density =< 0.99: go to 8, else go to 9\n",
      "   |        |\n",
      "   |        |     |->11  White\n",
      "   |        |     |\n",
      "   |        |  |->10 then if Residual sugar =< 1.10: go to 11, else go to 12\n",
      "   |        |  |  |\n",
      "   |        |  |  |  |->13  Red\n",
      "   |        |  |  |  |\n",
      "   |        |  |  |->12 else if Density =< 0.99: go to 13, else go to 14\n",
      "   |        |  |     |\n",
      "   |        |  |     |  |->15  Red\n",
      "   |        |  |     |  |\n",
      "   |        |  |     |->14 else if Chlorides =< 0.05: go to 15, else go to 16\n",
      "   |        |  |        |\n",
      "   |        |  |        |  |->17  Red\n",
      "   |        |  |        |  |\n",
      "   |        |  |        |->16 else if Sulphates =< 0.45: go to 17, else go to 18\n",
      "   |        |  |           |\n",
      "   |        |  |           |->18  Red\n",
      "   |        |  |\n",
      "   |        |->9 else if Residual sugar =< 10.70: go to 10, else go to 19\n",
      "   |           |\n",
      "   |           |->19  White\n",
      "   |\n",
      "if Total sulfur dioxide =< 66.50: go to 1, else go to 20\n",
      "   |\n",
      "   |                 |->26  Red\n",
      "   |                 |\n",
      "   |              |->25 then if Chlorides =< 0.01: go to 26, else go to 27\n",
      "   |              |  |\n",
      "   |              |  |        |->30  White\n",
      "   |              |  |        |\n",
      "   |              |  |     |->29 then if Citric acid =< 0.10: go to 30, else go to 31\n",
      "   |              |  |     |  |\n",
      "   |              |  |     |  |  |->32  White\n",
      "   |              |  |     |  |  |\n",
      "   |              |  |     |  |->31 else if Total sulfur dioxide =< 72.50: go to 32, else go to 33\n",
      "   |              |  |     |     |\n",
      "   |              |  |     |     |  |->34  White\n",
      "   |              |  |     |     |  |\n",
      "   |              |  |     |     |->33 else if Residual sugar =< 0.92: go to 34, else go to 35\n",
      "   |              |  |     |        |\n",
      "   |              |  |     |        |        |->38  White\n",
      "   |              |  |     |        |        |\n",
      "   |              |  |     |        |     |->37 then if Free sulfur dioxide =< 6.50: go to 38, else go to 39\n",
      "   |              |  |     |        |     |  |\n",
      "   |              |  |     |        |     |  |        |->42  White\n",
      "   |              |  |     |        |     |  |        |\n",
      "   |              |  |     |        |     |  |     |->41 then if pH =< 3.31: go to 42, else go to 43\n",
      "   |              |  |     |        |     |  |     |  |\n",
      "   |              |  |     |        |     |  |     |  |->43  White\n",
      "   |              |  |     |        |     |  |     |\n",
      "   |              |  |     |        |     |  |  |->40 then if Citric acid =< 0.23: go to 41, else go to 44\n",
      "   |              |  |     |        |     |  |  |  |\n",
      "   |              |  |     |        |     |  |  |  |->44  White\n",
      "   |              |  |     |        |     |  |  |\n",
      "   |              |  |     |        |     |  |->39 else if Sulphates =< 0.75: go to 40, else go to 45\n",
      "   |              |  |     |        |     |     |\n",
      "   |              |  |     |        |     |     |->45  White\n",
      "   |              |  |     |        |     |\n",
      "   |              |  |     |        |  |->36 then if Alcohol =< 13.58: go to 37, else go to 46\n",
      "   |              |  |     |        |  |  |\n",
      "   |              |  |     |        |  |  |->46  White\n",
      "   |              |  |     |        |  |\n",
      "   |              |  |     |        |->35 else if Sulphates =< 0.78: go to 36, else go to 47\n",
      "   |              |  |     |           |\n",
      "   |              |  |     |           |->47  White\n",
      "   |              |  |     |\n",
      "   |              |  |  |->28 then if Total sulfur dioxide =< 275.00: go to 29, else go to 48\n",
      "   |              |  |  |  |\n",
      "   |              |  |  |  |->48  White\n",
      "   |              |  |  |\n",
      "   |              |  |->27 else if Sulphates =< 0.81: go to 28, else go to 49\n",
      "   |              |     |\n",
      "   |              |     |->49  White\n",
      "   |              |\n",
      "   |           |->24 then if pH =< 3.83: go to 25, else go to 50\n",
      "   |           |  |\n",
      "   |           |  |->50  Red\n",
      "   |           |\n",
      "   |        |->23 then if Chlorides =< 0.07: go to 24, else go to 51\n",
      "   |        |  |\n",
      "   |        |  |->51  White\n",
      "   |        |\n",
      "   |     |->22 then if Sulphates =< 1.12: go to 23, else go to 52\n",
      "   |     |  |\n",
      "   |     |  |->52  Red\n",
      "   |     |\n",
      "   |  |->21 then if Volatile acidity =< 0.82: go to 22, else go to 53\n",
      "   |  |  |\n",
      "   |  |  |->53  Red\n",
      "   |  |\n",
      "   |->20 else if Chlorides =< 0.07: go to 21, else go to 54\n",
      "      |\n",
      "      |     |->56  Red\n",
      "      |     |\n",
      "      |  |->55 then if Citric acid =< 0.06: go to 56, else go to 57\n",
      "      |  |  |\n",
      "      |  |  |  |->58  Red\n",
      "      |  |  |  |\n",
      "      |  |  |->57 else if Chlorides =< 0.07: go to 58, else go to 59\n",
      "      |  |     |\n",
      "      |  |     |     |->61  Red\n",
      "      |  |     |     |\n",
      "      |  |     |  |->60 then if Total sulfur dioxide =< 68.50: go to 61, else go to 62\n",
      "      |  |     |  |  |\n",
      "      |  |     |  |  |->62  White\n",
      "      |  |     |  |\n",
      "      |  |     |->59 else if Fixed acidity =< 9.95: go to 60, else go to 63\n",
      "      |  |        |\n",
      "      |  |        |->63  Red\n",
      "      |  |\n",
      "      |->54 else if Density =< 1.00: go to 55, else go to 64\n",
      "         |\n",
      "         |        |->67  White\n",
      "         |        |\n",
      "         |     |->66 then if Volatile acidity =< 0.23: go to 67, else go to 68\n",
      "         |     |  |\n",
      "         |     |  |->68  Red\n",
      "         |     |\n",
      "         |  |->65 then if Total sulfur dioxide =< 157.50: go to 66, else go to 69\n",
      "         |  |  |\n",
      "         |  |  |->69  White\n",
      "         |  |\n",
      "         |->64 else if Residual sugar =< 6.25: go to 65, else go to 70\n",
      "            |\n",
      "            |->70  White\n",
      "<---------------------------------------------------------------------------------------->\n",
      "Tree Depth:  18\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import Toolbox.treeprint as tp\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini',min_samples_split=100)\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "tp.tree_print(clf,attributeNames,classNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.1.2\n",
    "The min_samples_split parameter determines the minimum amount of samples necessary in a node for it to be eligable for further splitting. By default it's set to 2, which means that the algorithm will split a node with 2 samples of differing classes into 2 new nodes with 1 entry. If we increase min_samples_split to 100 that wouldn't happen. Only a node with more than or equal to 100 samples with some difference would be split.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3\n",
    "\n",
    "(0.5 pts) Show that a wine with the following attribute values would be classified as white by the tree fitted in 3.1.2, by applying the `predict()` function from the DecisionTreeClassifier to this sample.  Have another look at the visualized tree. Which attributes are used to classify this wine? \n",
    "\n",
    "| #             | Attribute     | Value  |\n",
    "| ------------- |:-------------| -----:|\n",
    "|1 | Fixed acidity (tartaric) | 6.9 g/dm$^3$ |\n",
    "|2 | Volatile acidity (acetic) | 1.09 g/dm$^3$ |\n",
    "|3 | Citric acid | 0.06 g/dm$^3$ |\n",
    "|4 | Residual sugar | 2.1 g/dm$^3$ |\n",
    "|5 | Chlorides | 0.0061 g/dm$^3$ |\n",
    "|6 | Free sulfur dioxide | 12 mg/dm$^3$ |\n",
    "|7 | Total sulfur dioxide | 31 mg/dm$^3$ |\n",
    "|8 | Density | 0.99 g/cm$^3$ |\n",
    "|9 | pH | 3.5 |\n",
    "|10 | Sulphates | 0.64 g/dm$^3$ |\n",
    "|11 | Alcohol | 12 % vol. | \n",
    "\n",
    "** hints: **\n",
    "*If you don't know how to classify input values with a tree, see the help documentation for the DecisionTreeClassifier function on how to predict the label for a sample. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White\n"
     ]
    }
   ],
   "source": [
    "print(classNames[clf.predict([[6.9, 1.09, 0.06, 2.1, 0.0061, 12, 31, 0.99, 3.5, 0.64, 12]])[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.1.3\n",
    "The prediction shows that this wine is white, since 1 corresponds to the second entry in the classNames array, which is white. \n",
    "The tree splits that are relevant to this example are as follows in order:\n",
    "1. total sulfur dioxide =< 66.50\n",
    "2. chlorides =< 0.05\n",
    "3. sulphates > 0.56\n",
    "At this split it ends up in 4, which is an end-node for white wine.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4\n",
    "(1 pts) Classify all the wines in the wine data set. What percentage of the wine data is classified correctly by the tree?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of samples= 6304\n",
      "Amount of correctly classified samples= 6221\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for index in range(len(X)):\n",
    "    if(clf.predict([X[index]])[0] == y[index]):\n",
    "        correct += 1\n",
    "print(\"Amount of samples= \"+str(len(X)))\n",
    "print(\"Amount of correctly classified samples= \"+str(correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.1.4\n",
    "From the two values given by the above output we can calculate that the percentage of correctly classified wine samples by the tree is equal to: 6221/6304*100 ≈ 98.7%\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Decision tree pruning using cross-validation\n",
    "\n",
    "In this exercise we will use cross-validation to prune a decision tree. When applying cross-validation the observed data is split into training and test sets, i.e., `X_train`, `y_train` and `X_test` and `y_test`. We train the model on the training data and evaluate the performance of the trained model on the test data.\n",
    "\n",
    "#### 3.2.1\n",
    "(2 pts) We are again using the wine data set `Data/wine.mat`. Divide the data into a training and a test data set (see hints!). Fit a decision tree to the training data using the Gini (`gdi`) splitting criterion.\n",
    "\n",
    "Now, we want to find an optimally pruned decision tree by *modifying its maximum depth*. For different values of the parameter `depth` (from 2 to 20), \n",
    "+ first fit the decision tree\n",
    "+ then compute the classification error on the training and test set (this is called holdout cross-validation)\n",
    "\n",
    "When you've done this for `depth` values 2,...,20, plot the training and test classification error as a function of the tree depth in the same figure. This will show us what the optimal pruning depth is for the training set, but also how well this generalizes to a test set. \n",
    "\n",
    "\n",
    "** hints: **  \n",
    "*Take a look at the module `sklearn.model_selection` and see how it can be used to partition the data into a training and a test set (holdout validation, `train_test_split()` function). Note that the package also contains functions to partition data for K-fold cross-validation. Some of the functions can ensure that both training and test sets have roughly the same class proportions. The error is 1-accuracy. The easiest way to compute the accuracy is by using the sklearn metrics module: \n",
    " https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html *\n",
    "\n",
    "What appears to be the optimal tree depth? Do you get the same result when you run your code again, generating a new random split between training and test data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting tree for depth = 2\n",
      "\tClassification error on training set = 0.04262372720814589\n",
      "\tClassification error on testing set = 0.04901489668428638\n",
      "Fitting tree for depth = 3\n",
      "\tClassification error on training set = 0.02391664693345963\n",
      "\tClassification error on testing set = 0.031234983181162912\n",
      "Fitting tree for depth = 4\n",
      "\tClassification error on training set = 0.013734312100402546\n",
      "\tClassification error on testing set = 0.0211436809226333\n",
      "Fitting tree for depth = 5\n",
      "\tClassification error on training set = 0.011839924224484988\n",
      "\tClassification error on testing set = 0.017299375300336428\n",
      "Fitting tree for depth = 6\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 7\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 8\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 9\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 10\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 11\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 12\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 13\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 14\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 15\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 16\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 17\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 18\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 19\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n",
      "Fitting tree for depth = 20\n",
      "\tClassification error on training set = 0.011603125739995224\n",
      "\tClassification error on testing set = 0.016338298894762127\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "error_train = np.zeros(21);\n",
    "error_test = np.zeros(21);\n",
    "for depth in range(2,21):\n",
    "    # fit tree\n",
    "    print(\"Fitting tree for depth = \"+str(depth))\n",
    "    clf_train = tree.DecisionTreeClassifier(criterion='gini',min_samples_split=100, max_depth=depth)\n",
    "    clf_train = clf_train.fit(X_train, y_train)\n",
    "\n",
    "    # compute error\n",
    "    correct_train = 0;\n",
    "    correct_test = 0;\n",
    "    for index in range(len(X_train)):\n",
    "        if(index < len(X_train) and clf_train.predict([X_train[index]])[0] == y_train[index]):\n",
    "            correct_train += 1\n",
    "        if(index < len(X_test) and clf_train.predict([X_test[index]])[0] == y_test[index]):\n",
    "            correct_test += 1\n",
    "    \n",
    "    error_train[depth] = 1-correct_train/len(X_train)\n",
    "    error_test[depth] = 1-correct_test/len(X_test)\n",
    "    print(\"\\tClassification error on training set = \"+str(error_train[depth]))\n",
    "    print(\"\\tClassification error on testing set = \"+str(error_test[depth]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representation of the classification error in training and testing sets of the fitted tree at different depths.\n"
     ]
    }
   ],
   "source": [
    "# plot\n",
    "plt.plot(range(2,21), error_train[2:])\n",
    "plt.plot(range(2,21), error_test[2:])\n",
    "\n",
    "plt.xticks(np.arange(2, 21, step=1))\n",
    "plt.xlim([2, 20])\n",
    "\n",
    "plt.title(\"Classification error by depth\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.legend(['training classification error', 'testing classification error'], loc='upper right')\n",
    "plt.show()\n",
    "print(\"Representation of the classification error in training and testing sets of the fitted tree at different depths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.2.1\n",
    "From the plot we can see that the classification error for both datasets stagnates after depth 6. However, the result changes on every run of the code, because the split between training and test data is different. I would therefore say that depth 7 is the optimal depth for pruning. Any further splits yield no improvement in classification.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2\n",
    "(1.5 pts) Repeat the exercise above, using 10-fold cross-validation. K-fold cross-validation works by splitting X into K partitions of equal length. Then in each fold, you choose K-1 partitions as the training set and the remaining partition as the test set. Every partition gets a turn being a test set. For a very nice simple explanation of how (and why) K-fold works, check out http://www.dummies.com/programming/big-data/data-science/resorting-cross-validation-machine-learning/\n",
    "\n",
    "\n",
    "For each fold, fit a decision tree on the training set and evaluate its performance on the test set. Finally, compute the average classification errors on the test- and training sets across the 10 cross-validation folds and plot them as a function of the tree depth.\n",
    "\n",
    "** hints: **\n",
    "*This time the `KFold()` function from the module `sklearn.model_selection` can be used to partition the data into the 10 training and test partitions. Check out http://scikit-learn.org/stable/modules/cross_validation.html#k-fold for an example of how to acces the indices for the training set and the test set in each fold. Those indices can then be used to create the training and test set, e.g. `X_train = X[train_indices]`*\n",
    "\n",
    "*If you are creating a for loop iterating over an object where you both need the index and the elements of the object, you can enumerate the object. For example:*\n",
    "\n",
    "`\n",
    "for index, (element) in enumerate(object):\n",
    "    array[index] = operation(element)\n",
    "`\n",
    " \n",
    "\n",
    "What appears to be the optimal tree depth? Do you get the same result when you run your code again, generating a new random split between training and test data? How about 100-fold cross-validation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trees for depth = 2\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.039837828309536416\n",
      "\t\tClassification error on testing set = 0.3026941362916006\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.039309007579763744\n",
      "\t\tClassification error on testing set = 0.2266244057052298\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.035959809624537264\n",
      "\t\tClassification error on testing set = 0.03961965134706813\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.04406839414771724\n",
      "\t\tClassification error on testing set = 0.05229793977812991\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.04458935495241456\n",
      "\t\tClassification error on testing set = 0.04603174603174598\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.03894959464222769\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.037715897074374394\n",
      "\t\tClassification error on testing set = 0.020634920634920673\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.04758547761720122\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.03630595699682759\n",
      "\t\tClassification error on testing set = 0.03650793650793649\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.04441311244272117\n",
      "\t\tClassification error on testing set = 0.049206349206349254\n",
      "\n",
      "Average classification error on training set at depth 2 = 0.04087344333873213\n",
      "Average classification error on testing set at depth 2 = 0.08053631172490125\n",
      "\n",
      "Trees for depth = 3\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.019390093424995647\n",
      "\t\tClassification error on testing set = 0.15689381933438984\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.018508725542041193\n",
      "\t\tClassification error on testing set = 0.11727416798732171\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.019037546271813865\n",
      "\t\tClassification error on testing set = 0.047543581616481756\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.02432575356953992\n",
      "\t\tClassification error on testing set = 0.02694136291600635\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.025026436376453964\n",
      "\t\tClassification error on testing set = 0.031746031746031744\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.020972858653507176\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.022382798731053977\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.02731758900246739\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.02132534367289385\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.026436376454000654\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 3 = 0.022472352169876763\n",
      "Average classification error on testing set at depth 3 = 0.04534148366161043\n",
      "\n",
      "Trees for depth = 4\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.014806980433633044\n",
      "\t\tClassification error on testing set = 0.08399366085578452\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.010928961748633892\n",
      "\t\tClassification error on testing set = 0.1077654516640254\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.016217169046359947\n",
      "\t\tClassification error on testing set = 0.05546751188589538\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.01709853692931429\n",
      "\t\tClassification error on testing set = 0.01267828843106178\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.014628128304547072\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.018329221008107188\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019739161085653878\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.01568558336270709\n",
      "\t\tClassification error on testing set = 0.00952380952380949\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019386676066267206\n",
      "\t\tClassification error on testing set = 0.007936507936507908\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.017624250969333843\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 4 = 0.016444466895455746\n",
      "Average classification error on testing set at depth 4 = 0.034244459537644954\n",
      "\n",
      "Trees for depth = 5\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.013396791820906029\n",
      "\t\tClassification error on testing set = 0.07290015847860543\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008637405252952535\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015512074739996495\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.015159527586814714\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013746915756080336\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.018152978498413797\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019210433556573814\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.014099400775467008\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019210433556573814\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.015156855833627025\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 5 = 0.015228281737740556\n",
      "Average classification error on testing set at depth 5 = 0.03345357583075491\n",
      "\n",
      "Trees for depth = 6\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012867971091133468\n",
      "\t\tClassification error on testing set = 0.07131537242472263\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013394430736693663\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019210433556573814\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013394430736693663\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.014628128304547072\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 6 = 0.014840514042827957\n",
      "Average classification error on testing set at depth 6 = 0.03345382738409679\n",
      "\n",
      "Trees for depth = 7\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012867971091133468\n",
      "\t\tClassification error on testing set = 0.07131537242472263\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.014628128304547072\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 7 = 0.014787641289919972\n",
      "Average classification error on testing set at depth 7 = 0.03345382738409679\n",
      "\n",
      "Trees for depth = 8\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012867971091133468\n",
      "\t\tClassification error on testing set = 0.07131537242472263\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.014628128304547072\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 8 = 0.014787641289919972\n",
      "Average classification error on testing set at depth 8 = 0.03345382738409679\n",
      "\n",
      "Trees for depth = 9\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012867971091133468\n",
      "\t\tClassification error on testing set = 0.07131537242472263\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.014628128304547072\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 9 = 0.014787641289919972\n",
      "Average classification error on testing set at depth 9 = 0.03345382738409679\n",
      "\n",
      "Trees for depth = 10\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.014628128304547072\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 10 = 0.014734759216942705\n",
      "Average classification error on testing set at depth 10 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 11\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.014099400775467008\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 11 = 0.0146818864640347\n",
      "Average classification error on testing set at depth 11 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 12\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 12 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 12 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 13\n",
      "\tKfold = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 13 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 13 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 14\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 14 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 14 = 0.032978643121273875\n",
      "\n",
      "Trees for depth = 15\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 15 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 15 = 0.032978643121273875\n",
      "\n",
      "Trees for depth = 16\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 16 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 16 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 17\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 17 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 17 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 18\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 18 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 18 = 0.032819912962543704\n",
      "\n",
      "Trees for depth = 19\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 19 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 19 = 0.032978643121273875\n",
      "\n",
      "Trees for depth = 20\n",
      "\tKfold = 1\n",
      "\t\tClassification error on training set = 0.012339150361360796\n",
      "\t\tClassification error on testing set = 0.0649762282091918\n",
      "\tKfold = 2\n",
      "\t\tClassification error on training set = 0.008284858099770864\n",
      "\t\tClassification error on testing set = 0.10301109350237714\n",
      "\tKfold = 3\n",
      "\t\tClassification error on training set = 0.015335801163405605\n",
      "\t\tClassification error on testing set = 0.057052297939778174\n",
      "\tKfold = 4\n",
      "\t\tClassification error on training set = 0.014278159703860371\n",
      "\t\tClassification error on testing set = 0.009508716323296307\n",
      "\tKfold = 5\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.01746031746031751\n",
      "\tKfold = 6\n",
      "\t\tClassification error on training set = 0.017976735988720516\n",
      "\t\tClassification error on testing set = 0.01904761904761909\n",
      "\tKfold = 7\n",
      "\t\tClassification error on training set = 0.019034191046880533\n",
      "\t\tClassification error on testing set = 0.015873015873015928\n",
      "\tKfold = 8\n",
      "\t\tClassification error on training set = 0.013218188227000383\n",
      "\t\tClassification error on testing set = 0.012698412698412653\n",
      "\tKfold = 9\n",
      "\t\tClassification error on training set = 0.01885794853718714\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\tKfold = 10\n",
      "\t\tClassification error on training set = 0.013923158265773727\n",
      "\t\tClassification error on testing set = 0.014285714285714235\n",
      "\n",
      "Average classification error on training set at depth 20 = 0.014646637962096032\n",
      "Average classification error on testing set at depth 20 = 0.032819912962543704\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 10\n",
    "\n",
    "kf = KFold(n_splits=folds)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "avg_error_train = np.zeros(21);\n",
    "avg_error_test = np.zeros(21);\n",
    "for depth in range(2,21):\n",
    "    \n",
    "    print(\"\\nTrees for depth = \"+str(depth))\n",
    "    error_train_kfold = np.zeros(folds);\n",
    "    error_test_kfold = np.zeros(folds);\n",
    "        \n",
    "    for kfold_index, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        print(\"\\tKfold = \"+str(kfold_index+1))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "        # fit tree\n",
    "        clf_train = tree.DecisionTreeClassifier(criterion='gini',min_samples_split=100, max_depth=depth)\n",
    "        clf_train = clf_train.fit(X_train, y_train)\n",
    "\n",
    "        # compute error\n",
    "        correct_train = 0;\n",
    "        correct_test = 0;\n",
    "        for index in range(len(X_train)):\n",
    "            if(index < len(X_train) and clf_train.predict([X_train[index]])[0] == y_train[index]):\n",
    "                correct_train += 1\n",
    "            if(index < len(X_test) and clf_train.predict([X_test[index]])[0] == y_test[index]):\n",
    "                correct_test += 1\n",
    "\n",
    "        error_train_kfold[kfold_index] = 1-correct_train/len(X_train)\n",
    "        error_test_kfold[kfold_index] = 1-correct_test/len(X_test)\n",
    "        print(\"\\t\\tClassification error on training set = \"+str(error_train_kfold[kfold_index]))\n",
    "        print(\"\\t\\tClassification error on testing set = \"+str(error_test_kfold[kfold_index]))\n",
    "        \n",
    "        kfold_index+=1\n",
    "    \n",
    "    for error in error_train_kfold:\n",
    "        avg_error_train[depth] += error\n",
    "    for error in error_test_kfold:\n",
    "        avg_error_test[depth] += error\n",
    "        \n",
    "    avg_error_train[depth] /= folds\n",
    "    avg_error_test[depth] /= folds\n",
    "    print(\"\\nAverage classification error on training set at depth \"+str(depth)+\" = \"+str(avg_error_train[depth]))\n",
    "    print(\"Average classification error on testing set at depth \"+str(depth)+\" = \"+str(avg_error_test[depth]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average classification error for training and testing sets using K-fold cross-validation\n"
     ]
    }
   ],
   "source": [
    "# plot\n",
    "plt.plot(range(2,21), avg_error_train[2:])\n",
    "plt.plot(range(2,21), avg_error_test[2:])\n",
    "\n",
    "plt.xticks(np.arange(2, 21, step=1))\n",
    "plt.xlim([2, 20])\n",
    "\n",
    "plt.title(\"Average classification error by depth\")\n",
    "plt.xlabel(\"Depth\")\n",
    "plt.ylabel(\"Classification error\")\n",
    "plt.legend(['training classification error', 'testing classification error'], loc='upper right')\n",
    "plt.show()\n",
    "print(\"Average classification error for training and testing sets using K-fold cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.2.2\n",
    "In this case, after running the code a few times, it looks like the classification error in the test can still fluctuate up to depth 10, which is the optimal tree depth, looking to the above plot. When running the code again, just like 3.2.1, the result is different. 100-fold cross-validation resulted in more tiny fluctuation after depth 10 until depth 16, but these changes are so small (<1%) that for classification purposes it is negligible.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3.3 ROC curves, AUC scores, and the sign test\n",
    "\n",
    "In this exercise we will use ROC curves and the sign test to compare classifiers. Study the lecture slides and the paper 'ROC Graphs: Notes and Practical Considerations for Researchers' by Tom Fawcett included with the homework assignment (ROC101.pdf). It describes all you need to know (and much, much more..) about ROC curves. The method explained for computing the area under the curve is unnecessarily complicated. A simpler formula is:\n",
    "$$\n",
    "    \\mbox{AUC} = {1 \\over m n} \\sum_{i=1}^m \\sum_{j=1}^n \\mathbf{\\large 1}_{p_i > p_j} \\: .\n",
    "   $$\n",
    "Here $i$ runs over all $m$ data points with true label 1, and $j$ runs over all $n$ data points with true label $0$; $p_i$ and $p_j$ denote the probability score assigned by the classifier to data point $i$ and $j$, respectively. $\\mathbf{\\large 1}$ is the indicator function: it outputs $1$ if the condition (here $p_i > p_j$) is satisfied and $0$ otherwise. *Simply put: this formula computes how often the probability of a data point with true label 1 is higher than the probability of data points with true label 0. This is then divided by the total number of comparisons between probabilities.*\n",
    "    \n",
    "\n",
    "#### 3.3.1\n",
    "(0.25 pts) To make sure you understand how ROC works, make an ROC curve and calculate the AUC (on paper) with the following data:\n",
    "\n",
    "`\n",
    "labels = [0,0,0,1,0,1,1,0,1,1]\n",
    "classifier_probs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]\n",
    "`\n",
    "    \n",
    "What's the AUC you calculated? Do you think this classifier works well?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.3.1\n",
    "The AUC I calculated is 0.84. I think the classifier works quite well, since this is pretty close to 1 and way better than random guessing, which would have an AUC of 0.5.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2\n",
    "  Load the file `Data/classprobs.xls` using the Pandas `read_excel` function. Inspect the data and cast it to an array with the `.values` attribute. The first column gives the true class label (either 0 or 1). The second and third column give the probabilistic scores for two different classifiers. The higher this probability, the more certain the classifier is that the example belongs to class 1 (instead of class 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1.00000  0.88700   0.79706\n",
      "0        0  0.13591  0.099401\n",
      "1        0  0.55549  0.650750\n",
      "2        0  0.23876  0.508570\n",
      "3        0  0.14542  0.236290\n",
      "4        1  0.81876  0.489490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"Data/classprobs.xls\")\n",
    "print(data.head())\n",
    "X = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3\n",
    "(1 pts) Calculate the ROC curves for both of the classifiers by calculating the FP and TP rates and plot them in the same plot. Make the plot yourself as opposed to using a library! Also plot a dashed line on the diagonal to represent random guessing in the same figure. Interpret the obtained results. Do both classifiers perform better than random guessing?\n",
    "\n",
    "** hints: **  \n",
    "*The function `sklearn.metrics.roc_curve` can be used for computing the FP and TP rates. You can then plot them using the `matplotlib.pyplot` `plot` function. Check the documentation on how to plot a dashed line. Remember to include a legend!* \n",
    "\n",
    "**An extra tip: the sign test example below includes a figure with dashed lines and a legend...**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two ROC curves calculated from the classprobs.xls dataset\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(X[:,0], X[:,1])\n",
    "fpr2, tpr2, thresholds2 = metrics.roc_curve(X[:,0], X[:,2])\n",
    "\n",
    "plt.plot(fpr1, tpr1)\n",
    "plt.plot(fpr2, tpr2)\n",
    "plt.plot([0,1],[0,1])\n",
    "\n",
    "plt.title(\"ROC curves\")\n",
    "plt.xlabel(\"FP-rate\")\n",
    "plt.ylabel(\"TP-rate\")\n",
    "plt.legend(['first classifiers', 'second classifiers', 'random guessing'], loc='lower right')\n",
    "plt.show()\n",
    "print(\"The two ROC curves calculated from the classprobs.xls dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.3.3\n",
    "From the above graph, it looks like both classifiers perform better than random guessing. The first classifiers (blue line), the second column in the dataset, performs significantly better than the second classifiers.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4\n",
    "(0.5 pts) Compute the AUC scores (area under the curve) of both classifiers using the formula given above. **Write your own code for this calculation.** Do the AUC scores indicate that the classifiers are performing better than this baseline?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC1 = 0.9556254367575122\n",
      "AUC2 = 0.7613556953179594\n"
     ]
    }
   ],
   "source": [
    "AUC1 = AUC2 = m_length = 0\n",
    "\n",
    "for i in range(len(X)):\n",
    "    if(X[i,0] == 1):\n",
    "        m_length += 1\n",
    "        for j in range(len(X)):\n",
    "            if X[j,0] == 0:\n",
    "                if X[i,1] > X[j,1]:\n",
    "                    AUC1 += 1\n",
    "                if X[i,2] > X[j,2]:\n",
    "                    AUC2 += 1\n",
    "\n",
    "mn = m_length*(len(X)-m_length)\n",
    "AUC1 /= mn\n",
    "AUC2 /= mn\n",
    "\n",
    "print(\"AUC1 = \"+str(AUC1))\n",
    "print(\"AUC2 = \"+str(AUC2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.3.4\n",
    "AUC1 > AUC2, so the AUC scores confirm that the classifiers both perform better than the baseline of random guessing with AUC=0.5.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.3.5\n",
    "(0.25 pts) Using a threshold of 0.5, translate the probability scores to predicted class labels, and compute the accuracy for each of the classifiers.\n",
    "\n",
    "** hint: **\n",
    "\n",
    "*Take a look at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction of classifiers 1 = 0.8598130841121495\n",
      "Accuracy of prediction of classifiers 2 = 0.6915887850467289\n"
     ]
    }
   ],
   "source": [
    "predict1 = predict2 = np.array([])\n",
    "\n",
    "for label, p1, p2 in X:\n",
    "    if p1>=0.5:\n",
    "        predict1 = np.append(predict1, 1)\n",
    "    else:\n",
    "        predict1 = np.append(predict1, 0)\n",
    "    if p2>=0.5:\n",
    "        predict2 = np.append(predict2, 1)\n",
    "    else:\n",
    "        predict2 = np.append(predict2, 0)\n",
    "        \n",
    "acc1 = metrics.accuracy_score(X[:,0], predict1)\n",
    "acc2 = metrics.accuracy_score(X[:,0], predict2)\n",
    "print(\"Accuracy of prediction of classifiers 1 = \"+str(acc1))\n",
    "print(\"Accuracy of prediction of classifiers 2 = \"+str(acc2))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.6\n",
    "(1 pts) Create the 2x2 table (or array, or list, or dataframe..) of wrong and correct classifications by the first and the second classifier, like in the lecture slides (lecture on model evaluation, part on the sign test). Perform a sign test to test whether the performance of the two classifiers is significantly different at a significance level of 0.05. **If you are not familiar with sign testing, you can read the example provided below.**\n",
    "\n",
    "\n",
    "** hints: **  \n",
    "*As explained in the lecture slides, the sign test is a binomial test on the lower-left ($N_{1<2}$) and upper-right ($N_{1>2}$) elements of the cross table. Unlike in the lecture slides, here you need to perform a two-sided test: $$\\textrm{p-value} = P(W \\leq \\min(N_{1<2},N_{1>2}) \\textrm{ or } W \\geq  \\max(N_{1<2},N_{1>2})$$\n",
    "The function `scipy.stats.binom.cdf` can be used to compute the cumulative density of the binomial distribution.\n",
    "For more information on the sign test, see the included paper by Salzberg (signtest.pdf, in particular section 3.1).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[66. 26.]\n",
      " [ 8.  7.]]\n",
      "P-value = 0.0029350556433200854\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "# create array\n",
    "signArray = np.zeros((2,2))\n",
    "for i in range(len(X)):\n",
    "    if(predict1[i] == predict2[i] == X[i,0]):\n",
    "        signArray[0,0] += 1\n",
    "    elif(predict1[i] != predict2[i] == X[i,0]):\n",
    "        signArray[1,0] += 1\n",
    "    elif(predict2[i] != predict1[i] == X[i,0]):\n",
    "        signArray[0,1] += 1\n",
    "    elif(predict1[i] == predict2[i] != X[i,0]):\n",
    "        signArray[1,1] += 1\n",
    "print(signArray)\n",
    "\n",
    "# sign test\n",
    "N_r = signArray[0,1]\n",
    "N_l = signArray[1,0]\n",
    "b = binom(N_r+N_l,0.5)\n",
    "p = b.cdf(min(N_r,N_l)) + (1-b.cdf(max(N_r,N_l)-1))\n",
    "\n",
    "print(\"P-value = \"+str(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Answer 3.3.6\n",
    "The P-value is about 0.003, which is quite a bit less than our significance level of 0.05. Therefore, we have a significant difference between the two models. The first classifiers are better than the second.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example two-sided sign test\n",
    "\n",
    "Imagine you have a lucky coin, that you are certain does not result in heads 50% of the times you flip it. However, you friend (lets call him *zero_hypothesis*) claims that your coin is just like his random coin. To verify whether this is true, you decide to compare the performance (you are a scientist after all) of your lucky coin resulting in heads to the performance of his random coin resulting in heads with a sign test. How do you do this?\n",
    "\n",
    "First, you collect data. Assume you place 23 bets. During those 23 bets, you both flip your respective coins. It was determined that if one of you flipped head while the other didn't, that person won the bet, and the other person lost. If you both got the same outcome, neither of you won. 1 stands for head, and 0 for tails. These were your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------lucky | random \n",
      " bet 0      | 1 | 0\n",
      " bet 1      | 1 | 1\n",
      " bet 2      | 1 | 0\n",
      " bet 3      | 0 | 1\n",
      " bet 4      | 1 | 0\n",
      " bet 5      | 1 | 1\n",
      " bet 6      | 1 | 0\n",
      " bet 7      | 1 | 0\n",
      " bet 8      | 1 | 0\n",
      " bet 9      | 0 | 1\n",
      " bet 10     | 1 | 0\n",
      " bet 11     | 1 | 1\n",
      " bet 12     | 1 | 0\n",
      " bet 13     | 1 | 1\n",
      " bet 14     | 0 | 1\n",
      " bet 15     | 1 | 1\n",
      " bet 16     | 1 | 1\n",
      " bet 17     | 0 | 0\n",
      " bet 18     | 1 | 0\n",
      " bet 19     | 1 | 0\n",
      " bet 20     | 1 | 0\n",
      " bet 21     | 0 | 0\n",
      " bet 22     | 1 | 0\n",
      " -----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#results\n",
    "lucky_coin = np.array([1,1,1,0,1,1,1,1,1,0,1,1,1,1,0,1,1,0,1,1,1,0,1])\n",
    "random_coin= np.array([0,1,0,1,0,1,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,0,0])\n",
    "\n",
    "print('----------lucky | random \\n',*['bet {:<5}  | {} | {}\\n'.format(t[0],t[1],t[2]) for t in zip(range(0,len(lucky_coin)),lucky_coin,random_coin)],'-----\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you calculate how often in any pair of outcomes one coin *won*, i.e. one coin flipped to head while the other flipped to tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lucky coin won 12 times, while the random won 3 times.\n"
     ]
    }
   ],
   "source": [
    "N_l = sum((lucky_coin == 1) & (random_coin == 0)) #how often your lucky coin won\n",
    "N_r = sum((lucky_coin == 0) & (random_coin == 1)) #how often the random coin won\n",
    "N = N_l + N_r #how often one coin was a winner\n",
    "print('The lucky coin won {} times, while the random won {} times.'.format(N_l,N_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You observed 15 times that one coin won. If both coins resulted in heads 50% of the time, like *zero_hypothesis* stated, you'd expect that both coins have 50% chance of being the winner. So the probability distributions for both N_l and N_r should look something like this, assuming that *zero_hypothesis* is right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = binom(N,0.5) #N=observations of inequality, p=prob of one coin being better, which is 0.5 according to your H0\n",
    "plt.plot(b.pmf(range(N+1)), '--o',label = 'Probability density')\n",
    "plt.plot(b.cdf(range(N+1)), '--o',label = 'Cumulative distribution')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of times one coin is better than the other')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution according to H0')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course this is a discrete probability function, so you're not interested in values between integers.)\n",
    "\n",
    "As you can see, according to *zero_hypothesis*, 7 and 8 are the most likely values for both N_l and N_r. But this is not what you observed in your data. To conclude that *H0* is false, you need to show that your outcome is unlikely under the assumption that *H0* is true: the values you observed are so extreme that these values or more extreme values occur less than 5% of the time (*p-value < 0.05*). So how unlikely is it that one coin is better 12 times or more, and the other is better 3 times or less?\n",
    "\n",
    "It's the sum of the values of the probability density function (pdf or pmf in the python doc) at  0,1,2 and 3, plus the sum of the values of the pdf at 12,13,14 and 15. \n",
    "\n",
    "This is equal to the value of the cumulative distribution function (cdf) at 3, plus 1 minus the value of cdf at 11 (since `1-cdf(11)` represents the probability of observing 12 or bigger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value is 0.035\n"
     ]
    }
   ],
   "source": [
    "p_value = b.cdf(min(N_r,N_l)) + (1-b.cdf(max(N_r,N_l)-1))\n",
    "print('The p-value is {:.3f}'.format(p_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So your observations are very unlikely if *zero_hypothesis* is right. By statistical theory, you can now say that you have significant reason to reject their opinion. Now the hardest part of statistics: convincing your non-scientist friends."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
